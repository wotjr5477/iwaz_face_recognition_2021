{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73dad6b6-8a57-424a-81e2-598eddd083d7",
   "metadata": {},
   "source": [
    "# 1. 학습데이터 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553996a7-bb75-4e8c-b6a6-d9b28ae314de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1-1. 이름을 입력받아서 분류 수집 : OpenCV2-HaarCascade\n",
    "이름을 입력 받아서 해당 이름으로 이미 디렉토리가 있으면 해당 디렉토리 사용/없는 경우에는 새로운 디렉토리를 생성해서 해당 디렉토리 내에 사진 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928114f-efba-488d-9a6e-e79f8d17a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   opencv python 코딩\n",
    "#   얼굴을 인식한 상태에만 자동으로 촬영\n",
    "#\n",
    "\n",
    "import cv2\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# opencv python 코딩 기본 틀\n",
    "# 카메라 영상을 받아올 객체 선언 및 설정(영상 소스, 해상도 설정)\n",
    "capture = cv2.VideoCapture(0)\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "\n",
    "is_record = False   # 녹화상태 : 녹화중 x\n",
    "on_record = False\n",
    "cnt_record = 0      # 영상 녹화 시간 관련 변수\n",
    "max_cnt_record = 1  # 최소 촬영시간\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')    # 영상을 기록할 코덱 설정\n",
    "#font = ImageFont.truetype('fonts/ARIAL.TTF', 20) # 글꼴파일을 불러옴\n",
    "\n",
    "# haar cascade 검출기 객체 선언\n",
    "# face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "#eye_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_eye_tree_eyeglasses.xml')\n",
    "\n",
    "# 이름 입력 받기\n",
    "name = input(\"Enter Name :\") # 영어로\n",
    "\n",
    "# 디렉토리 생성 - 이미 존재할 경우 해당 디렉토리 사용\n",
    "output_path = os.getcwd() + \"/capture/\" + name + \"/\"\n",
    "print(output_path)\n",
    "os.makedirs(output_path, exist_ok = True)\n",
    "\n",
    "\n",
    "# 무한루프\n",
    "while True:\n",
    "    # 현재시각을 불러와 문자열로저장\n",
    "    now = datetime.datetime.now()\n",
    "    nowDatetime = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    nowDatetime_path = now.strftime('%Y-%m-%d %H_%M_%S') # 파일이름으로는 :를 못쓰기 때문에 따로 만들어줌\n",
    "\n",
    "    ret, frame = capture.read()     # 카메라로부터 현재 영상을 받아 frame에 저장, 잘 받았다면 ret가 참\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # 영상을 흑백으로 바꿔줌\n",
    "    \n",
    "    # 글자가 잘보이도록 배경을 넣어줌\n",
    "    # img는 사각형을 넣을 이미지, pt1, pt2는 사각형의 시작점과 끝점, color는 색상(파랑,초록,빨강), tickness는 선굵기(-1은 내부를 채우는 것)\n",
    "    cv2.rectangle(img=frame, pt1=(10, 15), pt2=(340, 35), color=(0,0,0), thickness=-1)     \n",
    "    frame = Image.fromarray(frame)    \n",
    "    draw = ImageDraw.Draw(frame)    \n",
    "    # xy는 텍스트 시작위치, text는 출력할 문자열, font는 글꼴, fill은 글자색(파랑,초록,빨강)   \n",
    "#     draw.text(xy=(10, 15),  text=\"camera \"+nowDatetime, font=font, fill=(255, 255, 255))\n",
    "    draw.text(xy=(10, 15),  text=\"camera \"+nowDatetime, fill=(255, 255, 255))\n",
    "    frame = np.array(frame)\n",
    "    \n",
    "    # scaleFactor를 1에 가깝게 해주면 정확도가 상승하나 시간이 오래걸림\n",
    "    # minNeighbors를 높여주면 검출률이 상승하나 오탐지율도 상승\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor= 1.5, minNeighbors=3, minSize=(20,20))\n",
    "    # print(faces)\n",
    "    \n",
    "    # 찾은 얼굴이 있으면\n",
    "    if len(faces) :\n",
    "        is_record = True    # 녹화 중\n",
    "        if on_record == False: # \n",
    "            #video = cv2.VideoWriter(\"capture/capture\" + nowDatetime_path + \".avi\", fourcc, 1, (frame.shape[1], frame.shape[0]))\n",
    "#             cv2.imwrite(os.getcwd() + \"/capture/capture\" + nowDatetime_path + \".jpg\", frame) # 프레임 캡쳐\n",
    "#             cv2.imwrite(\"C:/Users/CPB06GameN/Python/face_recognition/capture/재석/capture\" + nowDatetime_path + \".jpg\", frame) # 프레임 캡쳐\n",
    "#             cv2.imwrite(os.getcwd() + \"/capture/\" + name + \"/capture\" + nowDatetime_path + \".jpg\", frame) # 프레임 캡쳐\n",
    "            cv2.imwrite(output_path + \"capture\" + nowDatetime_path + \".jpg\", frame) # 프레임 캡쳐\n",
    "    \n",
    "        cnt_record = max_cnt_record\n",
    "        \n",
    "    if is_record == True:   # 녹화중이면\n",
    "        print('녹화 중')\n",
    "        #video.write(frame)    # 현재 프레임 저장\n",
    "        cnt_record = cnt_record - 1     # 녹화시간 1 감소\n",
    "        on_record = True    # 녹화중 여부를 참으로\n",
    "    if cnt_record == 0:     # 녹화시간이 다 되면\n",
    "        is_record = False   # 녹화관련 변수들을 거짓으로\n",
    "        on_record = False\n",
    "    \n",
    "    \n",
    "    # 얼굴 영역을 영상에 사각형으로 표시\n",
    "    if len(faces) :\n",
    "        for  x, y, w, h in faces :\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255,255,255), 2, cv2.LINE_4)\n",
    "    cv2.imshow(\"original\", frame)   # frame(카메라 영상)을 original 이라는 창에 띄워줌 \n",
    "    if cv2.waitKey(1000) == ord('q'):  # 키보드의 q 를 누르면 무한루프가 멈춤\n",
    "            break\n",
    "\n",
    "capture.release()                   # 캡처 객체를 없애줌\n",
    "cv2.destroyAllWindows()             # 모든 영상 창을 닫아줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de1276-f3a8-4c5a-b128-d86b6a296bf5",
   "metadata": {},
   "source": [
    "# 2. 학습데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421702ed-d57f-4021-8f84-0cf59e729cc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2-1. 얼굴 정보 수집 : face_landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a3891-5fa1-46b4-83c8-a808f30eec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "RIGHT_EYE = list(range(36, 42))\n",
    "LEFT_EYE = list(range(42, 48))\n",
    "MOUTH = list(range(48, 68))\n",
    "NOSE = list(range(27, 36))\n",
    "EYEBROWS = list(range(17, 27))\n",
    "JAWLINE = list(range(1, 17))\n",
    "ALL = list(range(0, 68))\n",
    "EYES = list(range(36, 48))\n",
    "\n",
    "predictor_file = 'model/shape_predictor_68_face_landmarks.dat'\n",
    "image_file = 'dataset/UZ/11.jpg'\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_file)\n",
    "\n",
    "image = cv2.imread(image_file)\n",
    "image_resize = cv2.resize(image, dsize=(700,700), interpolation = cv2.INTER_AREA) #사이즈 수정\n",
    "gray = cv2.cvtColor(image_resize, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "start_time = time.time()\n",
    "rects = detector(gray, 2) # 5>시간오래 걸리고 과탐지\n",
    "end_time = time.time()\n",
    "detect_time = end_time - start_time\n",
    "print(\"Number of faces detected: {}\".format(len(rects)))\n",
    "print(\"=== Detection took {:.3f} seconds\".format(detect_time))\n",
    "\n",
    "for (i, rect) in enumerate(rects):\n",
    "    start_time = time.time()\n",
    "\n",
    "    points = np.matrix([[p.x, p.y] for p in predictor(gray, rect).parts()])\n",
    "    show_parts = points[ALL]\n",
    "    for (i, point) in enumerate(show_parts):\n",
    "        x = point[0,0]\n",
    "        y = point[0,1]\n",
    "        #cv2.circle(image, (x, y), 1, (0, 255, 255), -1)\n",
    "        cv2.circle(image_resize, (x, y), 1, (0, 255, 255), -1)\n",
    "        #cv2.putText(image, \"{}\".format(i + 1), (x, y - 2),\n",
    "\t\t#cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1)\n",
    "        cv2.putText(image_resize, \"{}\".format(i + 1), (x, y - 2),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1)\n",
    "    end_time = time.time()\n",
    "    process_time = end_time - start_time\n",
    "    print(\"=== A frame took {:.3f} seconds\".format(process_time))\n",
    "\n",
    "cv2.imshow(\"Face Landmark\", image_resize)\n",
    "cv2.waitKey(0)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2979682-83cc-42c0-a809-fbc5c72f7631",
   "metadata": {},
   "source": [
    "## 2-2. 얼굴 사진 전처리 : face_alignment, face_landmark\n",
    "face_landmark를 통해 얻은 얼굴 정보에서 눈(eyes)에 해당하는 값들을 numpy_arctan함수를 이용하여 기울기를 계산하고 face_alignment를 사용하여 수평에 맞게 보정 및 기본 cv2함수로 얼굴만 정해놓은 사이즈로 잘라냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d67a28-d457-4947-8639-fe8923cd344f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2-2-1. ROI(Region of Interest) 설정 : face_landmark, face_alignment\n",
    "보정된 사진을 이용하여 정확도를 높이고 최적화를 위해서 관심영역을 얼굴로 지정하고 해당 영역만 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c6a310-4656-4f7a-96a9-8e71dac1434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[460 489]] [[705 385]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "#from google.colab.patches import cv2_imshow #coloab\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "RIGHT_EYE = list(range(36, 42))\n",
    "LEFT_EYE = list(range(42, 48))\n",
    "EYES = list(range(36, 48))\n",
    "\n",
    "predictor_file = 'model/shape_predictor_68_face_landmarks.dat'\n",
    "image_file = 'dataset/MZ/1.jpg'\n",
    "MARGIN_RATIO = 1.5\n",
    "OUTPUT_SIZE = (300, 300)\n",
    "\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_file)\n",
    "\n",
    "image = cv2.imread(image_file)\n",
    "image_origin = image.copy()\n",
    "\n",
    "(image_height, image_width) = image.shape[:2]\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "rects = detector(gray, 1)\n",
    "\n",
    "def getFaceDimension(rect):\n",
    "    return (rect.left(), rect.top(), rect.right() - rect.left(), rect.bottom() - rect.top())\n",
    "\n",
    "def getCropDimension(rect, center):\n",
    "    width = (rect.right() - rect.left())\n",
    "    half_width = width // 2\n",
    "    (centerX, centerY) = center\n",
    "    startX = centerX - half_width\n",
    "    endX = centerX + half_width\n",
    "    startY = rect.top()\n",
    "    endY = rect.bottom() \n",
    "    return (startX, endX, startY, endY)    \n",
    "\n",
    "for (i, rect) in enumerate(rects):\n",
    "    (x, y, w, h) = getFaceDimension(rect)\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    points = np.matrix([[p.x, p.y] for p in predictor(gray, rect).parts()])\n",
    "    show_parts = points[EYES]\n",
    "\n",
    "    right_eye_center = np.mean(points[RIGHT_EYE], axis = 0).astype(\"int\")\n",
    "    left_eye_center = np.mean(points[LEFT_EYE], axis = 0).astype(\"int\")\n",
    "    print(right_eye_center, left_eye_center)\n",
    " \n",
    "    cv2.circle(image, (right_eye_center[0,0], right_eye_center[0,1]), 5, (0, 0, 255), -1)\n",
    "    cv2.circle(image, (left_eye_center[0,0], left_eye_center[0,1]), 5, (0, 0, 255), -1)\n",
    "    \n",
    "    cv2.circle(image, (left_eye_center[0,0], right_eye_center[0,1]), 5, (0, 255, 0), -1)\n",
    "    \n",
    "    cv2.line(image, (right_eye_center[0,0], right_eye_center[0,1]),\n",
    "             (left_eye_center[0,0], left_eye_center[0,1]), (0, 255, 0), 2)\n",
    "    cv2.line(image, (right_eye_center[0,0], right_eye_center[0,1]),\n",
    "         (left_eye_center[0,0], right_eye_center[0,1]), (0, 255, 0), 1)\n",
    "    cv2.line(image, (left_eye_center[0,0], right_eye_center[0,1]),\n",
    "         (left_eye_center[0,0], left_eye_center[0,1]), (0, 255, 0), 1)\n",
    "\n",
    "    eye_delta_x = right_eye_center[0,0] - left_eye_center[0,0]\n",
    "    eye_delta_y = right_eye_center[0,1] - left_eye_center[0,1]\n",
    "    degree = np.degrees(np.arctan2(eye_delta_y,eye_delta_x)) - 180\n",
    "\n",
    "    eye_distance = np.sqrt((eye_delta_x ** 2) + (eye_delta_y ** 2))\n",
    "    aligned_eye_distance = left_eye_center[0,0] - right_eye_center[0,0]\n",
    "    scale = aligned_eye_distance / eye_distance\n",
    "\n",
    "    eyes_center = ((left_eye_center[0,0] + right_eye_center[0,0]) // 2,\n",
    "            (left_eye_center[0,1] + right_eye_center[0,1]) // 2)\n",
    "    cv2.circle(image, eyes_center, 5, (255, 0, 0), -1)\n",
    "            \n",
    "    metrix = cv2.getRotationMatrix2D(eyes_center, degree, scale)\n",
    "    cv2.putText(image, \"{:.5f}\".format(degree), (right_eye_center[0,0], right_eye_center[0,1] + 20),\n",
    "     \t cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    warped = cv2.warpAffine(image_origin, metrix, (image_width, image_height),\n",
    "        flags=cv2.INTER_CUBIC)\n",
    "    \n",
    "    cv2.imshow(\"warpAffine\", warped) # 각도 회전\n",
    "    #cv2_imshow(warped) #colab\n",
    "\n",
    "    (startX, endX, startY, endY) = getCropDimension(rect, eyes_center)\n",
    "    croped = warped[startY:endY, startX:endX]\n",
    "    output = cv2.resize(croped, OUTPUT_SIZE)\n",
    "    cv2.imshow(\"output\", output) # 최종\n",
    "    #cv2_imshow(output) #colab\n",
    "    \n",
    "    # 현재시각을 불러와 문자열로저장\n",
    "    now = datetime.datetime.now()\n",
    "    nowDatetime = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    nowDatetime_path = now.strftime('%Y-%m-%d %H_%M_%S') # 파일이름으로는 :를 못쓰기 때문에 따로 만들어줌\n",
    "    #cv2.imwrite(\"dataset/aligned/MZ/\" + nowDatetime_path + \".jpg\", output)\n",
    "\n",
    "\n",
    "    for (i, point) in enumerate(show_parts):\n",
    "        x = point[0,0]\n",
    "        y = point[0,1]\n",
    "        cv2.circle(image, (x, y), 1, (0, 255, 255), -1)\n",
    "\n",
    "cv2.imshow(\"Face Alignment\", image)\n",
    "#cv2_imshow(image) #colab\n",
    "\n",
    "\n",
    "\n",
    "cv2.waitKey(0)   \n",
    "cv2.destroyAllWindows()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a73d08-3255-4005-8424-a70202da0e36",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2-2-2. 학습데이터 전처리 자동화\n",
    "이름이나 학번/사번을 입력받아 새로운 디렉토리를 생성하고 학습데이터를 폴더별로 조회하여 수집된 사진을 전처리 한 후 생성된 디렉토리에 자동으로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ae1ed-e85d-467d-a4cf-01bd50f7076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "RIGHT_EYE = list(range(36, 42))\n",
    "LEFT_EYE = list(range(42, 48))\n",
    "EYES = list(range(36, 48))\n",
    "\n",
    "dataset_paths = ['/dataset/JB/', '/dataset/MZ/', '/dataset/UZ/', '/dataset/kim min jae/','/dataset/son/','/dataset/tedy/']\n",
    "output_paths = ['/dataset/aligned/JB/', '/dataset/aligned/MZ/', '/dataset/aligned/UZ/', '/dataset/aligned/kim min jae/', '/dataset/aligned/son/', '/dataset/aligned/tedy/']\n",
    "image_type = '.jpg'\n",
    "\n",
    "predictor_file = './model/shape_predictor_68_face_landmarks.dat'\n",
    "MARGIN_RATIO = 1.5\n",
    "OUTPUT_SIZE = (300, 300)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_file)\n",
    "\n",
    "\n",
    "def getFaceDimension(rect):\n",
    "    return (rect.left(), rect.top(), rect.right() - rect.left(), rect.bottom() - rect.top())\n",
    "\n",
    "\n",
    "def getCropDimension(rect, center):\n",
    "    width = (rect.right() - rect.left())\n",
    "    half_width = width // 2\n",
    "    (centerX, centerY) = center\n",
    "    startX = centerX - half_width\n",
    "    endX = centerX + half_width\n",
    "    startY = rect.top()\n",
    "    endY = rect.bottom() \n",
    "    return (startX, endX, startY, endY)    \n",
    "\n",
    "for (i, dataset_path) in enumerate(dataset_paths):\n",
    "    output_path = output_paths[i]\n",
    "    \n",
    "    ## 파일(이미지 개수)\n",
    "    dataset_path = dataset_paths[i]\n",
    "#     print(dataset_path)\n",
    "#     wd = os.getcwd() + \"/\" + dataset_path\n",
    "    dataset_path = os.getcwd() + dataset_path\n",
    "    output_path = os.getcwd() + output_path\n",
    "#     print(wd)\n",
    "    \n",
    "    jpg_list = [file for file in os.listdir(dataset_path) if file.endswith(image_type)]\n",
    "    print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "    print(jpg_list)\n",
    "    number_images = len(jpg_list) # .jpg만\n",
    "    ##\n",
    "    \n",
    "    for idx in range(number_images):\n",
    "        input_file = dataset_path + str(idx+1) + image_type\n",
    "#         input_file = dataset_path + str(idx+1) + image_type\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(\"input path: \" + input_file)\n",
    "        # get RGB image from BGR, OpenCV format\n",
    "        image = cv2.imread(input_file)\n",
    "        if image is None:\n",
    "            print(\"load failed!!\\n\")\n",
    "            continue\n",
    "        image_origin = image.copy()\n",
    "\n",
    "        (image_height, image_width) = image.shape[:2]\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        rects = detector(gray, 1)\n",
    "\n",
    "        for (i, rect) in enumerate(rects):\n",
    "            (x, y, w, h) = getFaceDimension(rect)\n",
    "            if 'x' in locals():\n",
    "                print(\"face detected!!\")\n",
    "            else :\n",
    "                print(\"face not detected!!\")\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # face_landmark로 얼굴 탐지 및 랜드마크 좌표 획득\n",
    "            points = np.matrix([[p.x, p.y] for p in predictor(gray, rect).parts()])\n",
    "#             print(type(str(points.ndim)))\n",
    "#             print(\"face_landmark: \" + str(len(points)))            \n",
    "            show_parts = points[EYES]\n",
    "\n",
    "            right_eye_center = np.mean(points[RIGHT_EYE], axis = 0).astype(\"int\")\n",
    "            left_eye_center = np.mean(points[LEFT_EYE], axis = 0).astype(\"int\")\n",
    "\n",
    "            eye_delta_x = right_eye_center[0,0] - left_eye_center[0,0]\n",
    "            eye_delta_y = right_eye_center[0,1] - left_eye_center[0,1]\n",
    "            degree = np.degrees(np.arctan2(eye_delta_y,eye_delta_x)) - 180\n",
    "\n",
    "            eye_distance = np.sqrt((eye_delta_x ** 2) + (eye_delta_y ** 2))\n",
    "            aligned_eye_distance = left_eye_center[0,0] - right_eye_center[0,0]\n",
    "            scale = aligned_eye_distance / eye_distance\n",
    "\n",
    "            eyes_center = ((left_eye_center[0,0] + right_eye_center[0,0]) // 2,\n",
    "                    (left_eye_center[0,1] + right_eye_center[0,1]) // 2)\n",
    "                    \n",
    "            metrix = cv2.getRotationMatrix2D(eyes_center, degree, scale)\n",
    "\n",
    "            warped = cv2.warpAffine(image_origin, metrix, (image_width, image_height),\n",
    "                flags=cv2.INTER_CUBIC)\n",
    "\n",
    "            (startX, endX, startY, endY) = getCropDimension(rect, eyes_center)\n",
    "\n",
    "            croped = warped[startY:endY, startX:endX]\n",
    "            output = cv2.resize(croped, OUTPUT_SIZE)\n",
    "            #output = warped[startY:endY, startX:endX]\n",
    "            \n",
    "            output_file = output_path + str(idx+1) + image_type\n",
    "            print(\"output_path: \" + output_file)\n",
    "            print(\"saved!!\\n\")\n",
    "            cv2.imshow(output_file, output)\n",
    "            cv2.imwrite(output_file, output)\n",
    "        \n",
    "cv2.waitKey(0)   \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72dead9-33f6-412e-bf8e-24bf94b8f871",
   "metadata": {},
   "source": [
    "# 3. 얼굴 인식(감지 및 식별)\n",
    "머신의 성능 문제로 Google_Colab에서 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d118502-8984-4d6c-8fc8-12800500b7c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3-1. 데이터 학습 : pickle\n",
    "수집한 학습데이터에서 얼굴을 탐지한 후 해당 얼굴정보를 벡터이미지로 인코딩하여 pickle포맷으로 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67dd419-c0bc-44b9-b313-11d0acf21c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# dataset_paths = ['/drive/My Drive/face_recognition/dataset/aligned/JB/', '/drive/My Drive/face_recognition/dataset/aligned/MZ/', '/drive/My Drive/face_recognition/dataset/aligned/UZ/', '/drive/My Drive/face_recognition/dataset/aligned/kim min jae/', '/drive/My Drive/face_recognition/dataset/aligned/son/', '/drive/My Drive/face_recognition/dataset/aligned/tedy/']\n",
    "dataset_paths = ['/drive/My Drive/face_recognition/dataset/aligned/JB/', '/drive/My Drive/face_recognition/dataset/aligned/MZ/', '/drive/My Drive/face_recognition/dataset/aligned/UZ/']\n",
    "# names = ['Junseok', 'Minzi', 'Yujin', 'Minjae', 'Son', 'Tedy']\n",
    "names = ['Junseok', 'Minzi', 'Yujin']\n",
    "image_type = '.jpg'\n",
    "encoding_file = 'encodings.pickle'\n",
    "# Either cnn  or hog. The CNN method is more accurate but slower. HOG is faster but less accurate.\n",
    "model_method = 'cnn'\n",
    "\n",
    "# initialize the list of known encodings and known names\n",
    "knownEncodings = []\n",
    "knownNames = []\n",
    "\n",
    "# loop over the image paths\n",
    "for (i, dataset_path) in enumerate(dataset_paths):\n",
    "    # extract the person name from names\n",
    "    name = names[i]\n",
    "    dataset_path = dataset_paths[i]\n",
    "    dataset_path = os.getcwd() + dataset_path\n",
    "    \n",
    "    jpg_list = [file for file in os.listdir(dataset_path) if file.endswith(image_type)]\n",
    "    number_images = len(jpg_list)\n",
    "    print(\"=====================\" + name + \"=====================\")\n",
    "    print(str(number_images) + \"images found\")\n",
    "\n",
    "    for idx in range(number_images):\n",
    "        file_name = dataset_path + str(idx+1) + image_type\n",
    "        print(file_name)\n",
    "\n",
    "        # load the input image and convert it from BGR (OpenCV ordering)\n",
    "        # to dlib ordering (RGB)\n",
    "        image = cv2.imread(file_name)\n",
    "        if image is None:\n",
    "            print(\"load failed!!\\n\")\n",
    "            continue\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # detect the (x, y)-coordinates of the bounding boxes\n",
    "        # corresponding to each face in the input image\n",
    "        boxes = face_recognition.face_locations(rgb,\n",
    "            model=model_method)\n",
    "\n",
    "        # compute the facial embedding for the face\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "\n",
    "        # loop over the encodings\n",
    "        for encoding in encodings:\n",
    "            # add each encoding + name to our set of known names and\n",
    "            # encodings\n",
    "            # print(file_name, name, encoding)\n",
    "            print(\"file encoded!!\")\n",
    "            knownEncodings.append(encoding)\n",
    "            knownNames.append(name)\n",
    "        \n",
    "# Save the facial encodings + names to disk\n",
    "data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "f = open(encoding_file, \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b9b575-25df-4a29-9580-6077a115d316",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3-2. 얼굴인식 : face_recognition(cnn or hog)\n",
    "학습시킨 데이터를 통해 얼굴인식을 수행하고자 하는 대상 이미지를 설정한 후, face_recognition을 통해 인코딩된 데이터와 대상 이미지 속에서 탐지된 얼굴정보를 대조하여 얼굴을 식별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4303cba-7633-4fae-a048-a0ca73739c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import pickle\n",
    "import time\n",
    "from google.colab.patches import cv2_imshow #coloab\n",
    "\n",
    "image_file = '/content/drive/My Drive/face_recognition/image/10.jpg'\n",
    "encoding_file = 'encodings.pickle'\n",
    "unknown_name = 'Unknown'\n",
    "# Either cnn  or hog. The CNN method is more accurate but slower. HOG is faster but less accurate.\n",
    "model_method = 'hog'\n",
    "\n",
    "def detectAndDisplay(image):\n",
    "    start_time = time.time()\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # detect the (x, y)-coordinates of the bounding boxes corresponding\n",
    "    # to each face in the input image, then compute the facial embeddings\n",
    "    # for each face\n",
    "    boxes = face_recognition.face_locations(rgb,\n",
    "        model=model_method)\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "\n",
    "    # initialize the list of names for each face detected\n",
    "    names = []\n",
    "\n",
    "    # loop over the facial embeddings\n",
    "    for encoding in encodings:\n",
    "        # attempt to match each face in the input image to our known\n",
    "        # encodings\n",
    "        matches = face_recognition.compare_faces(data[\"encodings\"],\n",
    "            encoding)\n",
    "        name = unknown_name\n",
    "\n",
    "        # check to see if we have found a match\n",
    "        if True in matches:\n",
    "            # find the indexes of all matched faces then initialize a\n",
    "            # dictionary to count the total number of times each face\n",
    "            # was matched\n",
    "            matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "            counts = {}\n",
    "\n",
    "            # loop over the matched indexes and maintain a count for\n",
    "            # each recognized face face\n",
    "            for i in matchedIdxs:\n",
    "                name = data[\"names\"][i]\n",
    "                counts[name] = counts.get(name, 0) + 1\n",
    "\n",
    "            # determine the recognized face with the largest number of\n",
    "            # votes (note: in the event of an unlikely tie Python will\n",
    "            # select first entry in the dictionary)\n",
    "            name = max(counts, key=counts.get)\n",
    "        \n",
    "        # update the list of names\n",
    "        names.append(name)\n",
    "\n",
    "    # loop over the recognized faces\n",
    "    for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "        # draw the predicted face name on the image\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        color = (0, 255, 0)\n",
    "        line = 2\n",
    "        if(name == unknown_name):\n",
    "            color = (0, 0, 255)\n",
    "            line = 1\n",
    "            name = ''\n",
    "            \n",
    "        cv2.rectangle(image, (left, top), (right, bottom), color, line)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(image, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.75, color, line)\n",
    "\n",
    "    end_time = time.time()\n",
    "    process_time = end_time - start_time\n",
    "    print(\"=== A frame took {:.3f} seconds\".format(process_time))\n",
    "    # show the output image\n",
    "#    cv2.imshow(\"Recognition\", image)\n",
    "    cv2_imshow(image) #colab    \n",
    "    \n",
    "# load the known faces and embeddings\n",
    "data = pickle.loads(open(encoding_file, \"rb\").read())\n",
    "\n",
    "# load the input image\n",
    "image = cv2.imread(image_file)\n",
    "detectAndDisplay(image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aebcbcb-71d7-46fa-9c2c-69be2df3fcb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# db연동-cam (예시)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1311494-d793-4ed3-874a-f94f83e8093d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 출근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c0d80-d348-4774-9791-6fba9859c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#준석\n",
    "\n",
    "import cv2\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pymysql\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "conn = pymysql.connect(host='192.168.0.232', port = 9966, user='root', password='root123', db='hnu_iwaz', charset='utf8', )\n",
    "curs = conn.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "is_record = False   # 녹화상태 : 녹화중 x\n",
    "on_record = False\n",
    "cnt_record = 0      # 영상 녹화 시간 관련 변수\n",
    "max_cnt_record = 1  # 최소 촬영시간\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')    # 영상을 기록할 코덱 설정\n",
    "\n",
    "# haar cascade 검출기 객체 선언\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# 이름 입력 받기\n",
    "name = \"Junseok Ban\"\n",
    "\n",
    "isRecognition = False\n",
    "# 무한루프\n",
    "while True:\n",
    "    now = datetime.datetime.now()\n",
    "    now = now - datetime.timedelta(hours=7)\n",
    "    \n",
    "    nowDatetime = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    nowDatetime_path = now.strftime('%Y-%m-%d %H_%M_%S') # 파일이름으로는 :를 못쓰기 때문에 따로 만들어줌\n",
    "\n",
    "    ret, frame = capture.read()     # 카메라로부터 현재 영상을 받아 frame에 저장, 잘 받았다면 ret가 참\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # 영상을 흑백으로 바꿔줌\n",
    "    \n",
    "    cv2.rectangle(img=frame, pt1=(10, 15), pt2=(340, 35), color=(0,0,0), thickness=-1)     \n",
    "    frame = Image.fromarray(frame)    \n",
    "    draw = ImageDraw.Draw(frame)    \n",
    "    draw.text(xy=(10, 15),  text=\"camera \"+nowDatetime, fill=(255, 255, 255))\n",
    "    frame = np.array(frame)\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor= 1.5, minNeighbors=3, minSize=(20,20))\n",
    "    \n",
    "    if len(faces) :\n",
    "        for  x, y, w, h in faces :\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0,255,0), 2, cv2.LINE_4)\n",
    "            cv2.putText(frame, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "            global isRecongnition\n",
    "            if isRecognition == False:\n",
    "                sql = \"insert into face(name, time, type) values(\\'\" + name + \"\\', \\'\" + nowDatetime + \"\\', \\'출근\\')\"\n",
    "#                 sql = \"\"\"\n",
    "#                 insert into face(name, time, type) values('name', 'time.strftime('%Y-%m-%d %H:%M:%S')', '출근')\n",
    "#                 \"\"\"\n",
    "                global conn\n",
    "                global curs\n",
    "                curs.execute(sql)\n",
    "                conn.commit()\n",
    "                isRecognition = True\n",
    "    else:\n",
    "        isRecognition = False\n",
    "    cv2.imshow(\"original\", frame)   # frame(카메라 영상)을 original 이라는 창에 띄워줌 \n",
    "    if cv2.waitKey(1000) == ord('q'):  # 키보드의 q 를 누르면 무한루프가 멈춤\n",
    "            break\n",
    "\n",
    "capture.release()                   # 캡처 객체를 없애줌\n",
    "cv2.destroyAllWindows()             # 모든 영상 창을 닫아줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c2eea-7d3f-41b7-839b-b1f38c5be357",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 퇴근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1561da59-6e90-4c8d-bfc3-2f6e65d99611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#유진\n",
    "\n",
    "import cv2\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pymysql\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "conn = pymysql.connect(host='192.168.0.232', port = 9966, user='root', password='root123', db='hnu_iwaz', charset='utf8', )\n",
    "curs = conn.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "is_record = False   # 녹화상태 : 녹화중 x\n",
    "on_record = False\n",
    "cnt_record = 0      # 영상 녹화 시간 관련 변수\n",
    "max_cnt_record = 1  # 최소 촬영시간\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')    # 영상을 기록할 코덱 설정\n",
    "\n",
    "# haar cascade 검출기 객체 선언\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# 이름 입력 받기\n",
    "name = \"Yujin Kim\"\n",
    "\n",
    "isRecognition = False\n",
    "# 무한루프\n",
    "while True:\n",
    "    now = datetime.datetime.now()\n",
    "    now = now + datetime.timedelta(hours=2)\n",
    "    \n",
    "    nowDatetime = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    nowDatetime_path = now.strftime('%Y-%m-%d %H_%M_%S') # 파일이름으로는 :를 못쓰기 때문에 따로 만들어줌\n",
    "\n",
    "    ret, frame = capture.read()     # 카메라로부터 현재 영상을 받아 frame에 저장, 잘 받았다면 ret가 참\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # 영상을 흑백으로 바꿔줌\n",
    "    \n",
    "    cv2.rectangle(img=frame, pt1=(10, 15), pt2=(340, 35), color=(0,0,0), thickness=-1)     \n",
    "    frame = Image.fromarray(frame)    \n",
    "    draw = ImageDraw.Draw(frame)    \n",
    "    draw.text(xy=(10, 15),  text=\"camera \"+nowDatetime, fill=(255, 255, 255))\n",
    "    frame = np.array(frame)\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor= 1.5, minNeighbors=3, minSize=(20,20))\n",
    "    \n",
    "    if len(faces) :\n",
    "        for  x, y, w, h in faces :\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0,255,0), 2, cv2.LINE_4)\n",
    "            cv2.putText(frame, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "            global isRecongnition\n",
    "            if isRecognition == False:\n",
    "                sql = \"insert into face(name, time, type) values(\\'\" + name + \"\\', \\'\" + nowDatetime + \"\\', \\'퇴근\\')\"\n",
    "#                 sql = \"\"\"\n",
    "#                 insert into face(name, time, type) values('name', 'time.strftime('%Y-%m-%d %H:%M:%S')', '출근')\n",
    "#                 \"\"\"\n",
    "                global conn\n",
    "                global curs\n",
    "                curs.execute(sql)\n",
    "                conn.commit()\n",
    "                isRecognition = True\n",
    "    else:\n",
    "        isRecognition = False\n",
    "    cv2.imshow(\"original\", frame)   # frame(카메라 영상)을 original 이라는 창에 띄워줌 \n",
    "    if cv2.waitKey(1000) == ord('q'):  # 키보드의 q 를 누르면 무한루프가 멈춤\n",
    "            break\n",
    "\n",
    "capture.release()                   # 캡처 객체를 없애줌\n",
    "cv2.destroyAllWindows()             # 모든 영상 창을 닫아줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff828d-fa75-45f8-aedc-61252bf1dd53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a44861-1d3c-4b29-b4fb-4c9b44211a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060bf8a-afd4-4a34-a741-10a9592fa6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2c75c-eab2-42af-aecd-e8f9bd255275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43754d29-0cb4-4251-90d5-5e787faa01e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
